{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document fo vector using Word2Vec\n",
    "Word2Vec library is utilized to convert each word into a vector. Vector corresponding to a document is the average Word2Vec of all words that are present in the document and Word2Vec library. Each document vector is a N = 300 dimension. In this representation, semantically similar vectors remains close to each other in N dimensional space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document to Vector\n",
    "\n",
    "import gensim.models.keyedvectors as word2vec\n",
    "import numpy as np\n",
    "\n",
    "class DocumentToVector:\n",
    "\n",
    "    def __init__(self):\n",
    "        filename = 'GoogleNews-vectors-negative300.bin.gz'\n",
    "        self.__word2vec_model = word2vec.KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "        # Normalize vectors\n",
    "        self.__word2vec_model.init_sims(replace=True)\n",
    "\n",
    "    def getWord2Vec(self, word):\n",
    "        return self.__word2vec_model[word]\n",
    "\n",
    "\n",
    "    def getDoc2Vec(self, doc):\n",
    "        doc = [word for word in doc if word in self.__word2vec_model.vocab]\n",
    "        # Check if at least one word in the vocab is present in the doc\n",
    "        if(len(doc)):\n",
    "            return np.mean(self.__word2vec_model[doc], axis=0)\n",
    "        else:\n",
    "            return np.array([0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec = DocumentToVector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check just for one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object type: <class 'numpy.ndarray'>\n",
      "Doc vector length: 300\n",
      "Data type: float32\n"
     ]
    }
   ],
   "source": [
    "doc = '''\n",
    "Markov chain Monte Carlo (MCMC) methods are an important algorithmic\n",
    " device in a variety of fields.  This project studies techniques for rigorous\n",
    " analysis of the convergence properties of Markov chains.   The emphasis is on\n",
    " refining probabilistic, analytic and combinatorial tools (such as coupling,\n",
    " log-Sobolev, and canonical paths) to improve existing algorithms and develop\n",
    " efficient algorithms for important open problems.\n",
    " Problems arising in\n",
    " computer science, discrete mathematics, and physics are of particular interest,\n",
    " e.g., generating random colorings and independent sets of bounded-degree\n",
    " graphs, approximating the permanent, estimating the volume of a convex body,\n",
    " and sampling contingency tables.  The project also studies inherent connections\n",
    " between phase transitions in statistical physics models and convergence\n",
    " properties of associated Markov chains.\n",
    " The investigator is developing a\n",
    " new graduate course on MCMC methods.\n",
    "'''\n",
    "import pandas as pd\n",
    "doc_vec = doc2vec.getDoc2Vec(doc)\n",
    "print(f'Object type: {type(doc_vec)}')\n",
    "print(f'Doc vector length: {len(doc_vec)}')\n",
    "print(f'Data type: {doc_vec.dtype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process all the files in directory tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique identifier 'AwardNo' and abstract content is extracted from every file of NSF grant dataset: Part-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractExtrator:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extractAbstract(self, filepath):\n",
    "        line_index = 0\n",
    "        line_index_start = 0\n",
    "        with open(filepath, encoding='iso-8859-1') as f:  \n",
    "        #with open(filepath) as f:  \n",
    "            content = f.readlines()\n",
    "            for line in content:\n",
    "                #print(line)\n",
    "                line_index += 1\n",
    "                if line.find('Award Number:') != -1:\n",
    "                    awardNo = line.split(':')[1].strip()\n",
    "                if line.find('Abstract    :') != -1:\n",
    "                    break\n",
    "\n",
    "        #print('line_index:', line_index)\n",
    "        abstract = ' '.join(content[line_index:]).strip()\n",
    "        return abstract,awardNo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All documents with empty/null/Not-Available abstracts are filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_filter_(doc):\n",
    "    if (len(doc) == 0):\n",
    "        return None\n",
    "    \n",
    "    NotAvailable = \"Not Available\"\n",
    "    NotAvailable_Content_Len = 13  # Found by looking at raw data\n",
    "    # 'no' or 'Not available' or ***** found in abstract that need to exclude\n",
    "    #if (doc.find(NotAvailable) != -1 and len(doc) == NotAvailable_ContentLen):\n",
    "    if len(doc) < 20: # Very short documents\n",
    "        return None\n",
    "    \n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of valid files = 49074\n",
      "No of skipped files scanned = 2685\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from fnmatch import fnmatch\n",
    "pattern = \"*.txt\"\n",
    "\n",
    "def list_files(dir):\n",
    "    r = []\n",
    "    for root, dirs, files in os.walk(dir):\n",
    "        for name in files:\n",
    "            if fnmatch(name, pattern):\n",
    "                r.append(os.path.join(root, name))\n",
    "    return r\n",
    "\n",
    "files = list_files('./Part1/')\n",
    "#print(files))\n",
    "\n",
    "docs = []\n",
    "docs_discarded = []\n",
    "docs_vector = []\n",
    "\n",
    "no_files = 0\n",
    "no_skipped_files = 0\n",
    "for file in files:\n",
    "    #print(file)\n",
    "    # Extract Abstract\n",
    "    absExt = AbstractExtrator()\n",
    "    doc,award = absExt.extractAbstract(file)\n",
    "    #print('Award: ', award)\n",
    "    if award == 'null':\n",
    "        #print('Skipped due to null Award')\n",
    "        docs_discarded.append([award,doc])\n",
    "        continue\n",
    "    \n",
    "    # Skipping Blank Abstract\n",
    "    if(docs_filter_(doc) == None):\n",
    "        #print('Skipped due to blank Abstract')\n",
    "        no_skipped_files += 1\n",
    "        docs_discarded.append([award,doc])\n",
    "        continue\n",
    "    \n",
    "    docs.append([award,doc])\n",
    "    #Determine vector representation of the document\n",
    "    vec = doc2vec.getDoc2Vec(doc)\n",
    "    \n",
    "    if(len(vec) != 1):\n",
    "        vec = np.insert(vec, 0, award)\n",
    "        #print(vec)\n",
    "        docs_vector.append(vec)\n",
    "        no_files += 1\n",
    "        \n",
    "    else:\n",
    "       # print('Skipped due to poor vocab')\n",
    "        no_skipped_files += 1\n",
    "        docs_discarded.append([award,doc])\n",
    "        continue\n",
    "    \n",
    "    #break\n",
    "    \n",
    "print(f'No of valid files = {no_files}')\n",
    "print(f'No of skipped files scanned = {no_skipped_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of documents are chosen for processing: 49074"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vector = np.array(docs_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the document vectors into files for easy post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'docs_vector.csv' is a pre-processed file containing 49047 documents each of 100 dimension. The index is the award number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf = pd.DataFrame(docs_vector)\n",
    "principalDf.set_index(principalDf.columns[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf.to_csv('docs_vector.csv',header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the processed document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Award number and abstract of selected files are written in 'docs.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if docs are extracted properly\n",
    "import  csv\n",
    "with open(\"docs.txt\",\"w\") as f:\n",
    "    for item in docs:\n",
    "        #if(docs_filter_(doc) != None):\n",
    "        f.write(item[0])\n",
    "        f.write(\": \\n\")        \n",
    "        f.write(item[1])\n",
    "        f.write(\"\\n @@@@@*****@@@@@ \\n\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Award number and abstract of discarded files are written in 'docs_discarted.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if docs are extracted properly\n",
    "import  csv\n",
    "with open(\"docs_discarded.txt\",\"w\") as f:\n",
    "    for item in docs_discarded:\n",
    "        #if(docs_filter_(doc) != None):\n",
    "        f.write(item[0])\n",
    "        f.write(\": \\n\")        \n",
    "        f.write(item[1])\n",
    "        f.write(\"\\n @@@@@*****@@@@@ \\n\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Abstract:  Abstractions are the basis for most of the ways that programmers look          \n",
      "               at their programs, data, and execution.  Most of these abstractions            \n",
      "               have a visual component.  This work envisions a system that will allow         \n",
      "               programmers to rapidly define abstractions and their accompanying              \n",
      "               visualizations.  Such a system would have immediate applications to            \n",
      "               programming environments, data bases, parallel programming, and                \n",
      "               education.  This project will investigate what it means to design such         \n",
      "               a system and to explore many of the underlying problems that must be           \n",
      "               solved before such a system can become a reality.  In conjunction with         \n",
      "               other research projects, it will lead to an abstraction-based                  \n",
      "               visualization environment to experiment with various solutions to              \n",
      "               these problems and to evaluate the underlying concepts.\n",
      "Test awardNo:  9113226\n"
     ]
    }
   ],
   "source": [
    "# Testing random abstracts\n",
    "absExt1 = AbstractExtrator()\n",
    "test_abstract, test_awardNo = absExt1.extractAbstract('./Part1/awards_1991/awd_1991_13/a9113226.txt')\n",
    "print('Test Abstract: ', test_abstract)\n",
    "print('Test awardNo: ', test_awardNo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
